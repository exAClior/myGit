#+TITLE: Learning notes for ai on udacity


* <2018-11-27 Tue>

** Overfitting and underfitting 
   Overfitting: the model is too complicated (kill a fly with bazuka)
   underfitting: the model is over simplified and does not solve the problem correctly
   Most often, we will end up in either over complicated or over simplified model

**  How to stop overcomplicating model
    we have two groups of data of known result.
    Training set: used to train the model
    Testing set: test how well the model is performing 
    We pick the model which both testing and training set gives good error sizes

**  What leads to overfitting
    large coefficients (weights, that can be factored by the same number while still maining the separation line the same) usually leads to overfitting, one way to prevent this is to add the 
    size of the weight (abs value or squared) to the error function
    This is called Regularization

** Two kinds of regularization

*** L1 : abs value
    it tend to reduce small weights to zero, so, at the end of the day. we will only need to use fewer weights
    Good for feature selections, i.e  reduce number of weights you need to use

*** L2 : square 
    tend to maintian all weights small uniformly but no zero weights

** Dropout 
  Sometimes part of neural network are really well trained and gets more and more well trained
  to prevent this, each epoch every node will have a chance of been droppped out when being trained

** Local minima 
   Sometimes you end up in local minimum, to prevent this, use multiple random starting point and use gradient descent
   pick the min of them all.

** Vanishing gradient
   gradient gets very small since activation function gets slope very close to zero as we go to \pm infinity
   and these gradients gets multiplied together, this makes gradients get even smaller
   To prevent this, we use different activation function , hyperbolic tangent , or relu function

** Batch vs. Stochastic Gradient descent 
   Divide the large training sets into smaller batches, train each batch one after another for every epoch
   you will make small and not very good change to the weights but you will have a much faster performance.

** Learning rate decay
   If model is not learning correctly, decrease the learning rate

** Momentum 
   beta value between 0 and 1, multiply each the nth step before this step by beta to the power of n and add to the current step you will be taking 



* <2018-11-26 Mon>

** Find global min
   Use momentum [[https://distill.pub/2017/momentum/][Paper to momentum]]

** Multilayer Perceptrons
   To make a row vector column, arr[:,None]

** Links to further reading 
   [[https://www.youtube.com/watch?v%3D59Hbtz7XgjM][Stanford Backpropagation]]
   [[https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b][Blog about backpropagation]]

* <2018-11-25 Sun>

** Non-linear models

***  How to find the curve
     Combine two linear model into a non-linear. Add the probability of being true in both with weights, and then feed it to the sigmoid function. You can even add a bias

***  How is this related to neural network
     this kind of combining two linear model into a nonlinear one

***  Deep Neural Network
     When you have multiple hidden layers you get a highly nonlinear boundary within n dimension space

** Feedforward
   Take input value and apply sigmoid func and do matrix multiplication and then output the value

*** Error Function
    remains the same as before

** Backpropogation
   This is how you train a neural network

*** Basically how you do
    You look at each point and see how each model is classifying it 
    add weight to the models that correctly identify the point and vice versa
    
* <2018-11-24 Sat>

** One-Hot Encoding 
   When you want to convert category data into numerical. The numerical input will be one or zero depending on whether this category is received by that subject or not

** Maximum Likelihood
   Each model assign each point two probabilities which adds up to one. (Correct : x , then incorrect is 1 -x). For each point pick the probability which correspond to the actual situation of the point
   i.e if the point is infact incorrect, and the incorrect probability assigned by the model is 0.3 then the P(point) = 0.3, multiply all this kind of probability together
   you get the likelihood correspond to this model. The model with highest lieklihood is the best model w.r.t this set of data

**  Maximizing Probabilities 
    But product is hard to compute, now, we use sum $-\sum_{i = 0}^{N }log(P(x_i))$

** Cross Entropy
   The above is called Cross Entropy, the lower the cross entropy, the better it is.
   To calculate Cross Entropy given two list Y (1 if for ith item, it happened, 0 otherwise)
   abd P (the probability when ith item happened)
   Cross Entropy = -\sum_{i}^{m} y_i \ln{p_i} + (1-y_1) \ln{p_i}

** Multi-Class Cross Entropy 
   if the item can be in different categories, then we simply have multiple Y lists and P lists (or in Comp Sci language, use matrix)
   Cross_Entropy = - \sum_{i = 1}^{n}\sum_{j = 1}^{m} y_{ij}ln(p_{ij})

** Logistic Regression
   replace all probabilities into sigmoid function \sigma(W,b), when we try to find the best model
   we use gradient descent to try and minimize the Cross Entropy by assigning different W and b

** Gradient Descent
   set a learning rate, and change W and b in the negative direction of the gradient
   So now, W'_i = W_i- \alpha \frac{d Error}{d w_i} 
   and     b' = b - \alpha \frac{d Error}{d b}

***  Derivative of Sigmoid function
     \sigma'(x) = \sigma(x)(1-\sigma(x))

***  To calculate gradient 
     use the sigmoid derivative formula and think of the error produced by all points as 
     the average of error produced by each point 
* <2018-11-07 Wed>

**  Perceptron trick
    How to move the line closer to a point. 
    say line is ax + by + bias
    and the misplaced point is (m,n) 
    the new a, b are 
    (a,b) + (-1)^k (m,n) * learning rate
    k is 0 for the should accept point misplaced
    k is 1 for the should not accept point misplaced

**  Perceptron  trick for more complicated line of seperation 
    Key point: in order for gradient descent to work, you cannot
    have a discrete error function but a continuous one

***  The solution is to use a Sigmoid function, with its argument as 
     WX+ b value we calculated

***  Softmax function
     when you have more than one class to be classified 
     you can use the softmax function to assign probability to each category 
     it works like a partition function 
* <2018-11-06 Tue>

** There is a radius of convergence for Taylor series out of which the 
   Taylor series do not be the same as the original function

**  What is a Neural Network
    Neuron, a thing that holds a number between 0 and 1, the number represen the activation

**  How to normalize the weighted sum, 
    use sigmoid function

**  How to train data
    Use gradient descent

**  Backpropagation 
    How we compute the gradient

***  We know what the current results are , and we know what change we should  have 
     in the previous layer of neurons, so we adjust weight accordingly 
     Or we change the activation of the previous layer neurons

***  You average all the nudges we need to weights and activations and every sample 
     this is the negative of the gradient

*** Stochastic backpropagate 
    randomly divide data groups into sub-batches , do backpropagate on each group
    to determine only one step in the gradient descent to the local min

**  Intro to Neural Networks
* <2018-11-03 Sat>

**  Artificial Neural Network
    Has three types of layers
    input layer , only 1
    output layer, only 1
    hidden layer, can have many 
    Note: numbers of input neurons , output neurons, and internal neurons can be different

**  How are Neurons Connected 
    each neuron on layer x is connected to all neurons on layer x+1
    the connecting lines have weights, these weights made up a weight matrix

**  How to train
    we adjust the weight matrices so that the output layer is as close as we want 
    based on information from the training data sets
    This optimization process could be broken down into two general steps 
** The feedforward process 
* <2018-11-02 Fri>

**  Linear Transformation 
    For a transformation to be linear, the origin has to stay there and 
    all the grid lines and diagonal lines in original coordinate 
    system has to remain staight lines

**  transformation matrix
    the first column is where the x hat will be after transformation
    the second is where y hat will be
    can be generalized into higher dimension I believe

* <2018-10-30 Tue>

**  
* <2018-10-27 Sat>

** Adaptation of Univariate Plots

*** use of barplot 
    instead of the counts we plot the mean of a value 
    we may also add error bars

*** Use of pointplot 
    if do not like bars, can use plot points to make things clear

*** adapted histogram 
    by changing the weight of each element can plot the density instead of count

**  line plots 
    want to emphasis the connection between change of one variable versus another 
    e.g stock value against time 
    use errorbar() note, need to sort the x values

***  Group continuous x variables 
     use bin edges 
     pd.cut()
     datafram.groupby()

***  may also achieve by using the rolling method in pandas' rolling window

[[https://stackoverflow.com/questions/20144529/shifted-colorbar-matplotlib][Good Color Map Reference]]

** Swarm plot
   same result of jittering , but the displacement is not random
   place dots as close to their original value as possible without overlap

** Rugplot
   displace density of points on axes

** Stripplot like 
* <2018-10-26 Fri>

**  scatterplots and correlation

***  correlation
     Pearson coefficient : r , how one variable changes w.r.t another

***  use of functions

****  scatter()
      plt.scatter(data = , x = , y = )

****  regplot()
      sb.regplot(data , x = , y = ) plots with regression line
  
** Overplotting Transparency and Jitter

***  Problem
****  Too many data points, the plot is now just a blob. 
      We sample part of the data and add transparency to the data dots 

**** Jitter
     When data points overlap and they take discrete values
     add random noise to their position in graph
     Too add transparency setting to regplot , need to pass it as one element
     of a dictionary of the variable scatter_kws

**  Heat Map
    Color denotes how many data points is in an area in the plot 
    hist2d() 
    keep an eye to the bin size

**  Violin Plots
    qualitative vs quantitative data 
    curve for each categorical group 
    seaborn.violonplot()

** Box plot 
   depicts outlier, max, q3 , median , q1 and min from top to bown 
   seaborn.boxplot()

** Cluster Bar Charts 
   seabon.heatmap()
   countplot() can also differentiate two different subgroup with in each column
   just provide with 'hue' parameter

** Faceting 
   different subset of data , create same kind of plot 
   remember to fix axes to be the same for these plots 
   g =  sb.FacetGrid()
   g.map () to determine what plot you need to do 
* <2018-10-25 Thu>

**  Scales and Transformations continuted 
* <2018-10-24 Wed>

**  Tidy data

*** Def
    each variable is a column
    each unit of observation is a row
    the observations of same units form a table

** Bar chart

***  depict the distribution of a categorical variable

***  we use matplotlib and seaborn to visualize data

***  countplot() to plot bar chars
     sb.countplot(data = variable_data, x/y = 'column_name')
     use [[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.api.types.CategoricalDtype.html][pandas.api.types.CategoricalDtype]] to generate an order categorical data 
     type made up of column names
     use plt.xticks(rotation = degrees) to rotate the x labels

** How to create Relative Frequency 
   one way is to change the label for frequency counts by manually calculating the 
   max frequency of one label on the other axis
   plt.yticks(values on y axis, label on that pos on y axis)


*** To maintain some sort of order 
    var_order = df['type_string'].value_counts().index
    returns a list of indexs for each row that are sorted by "type_string' value

***  Or you may display the percentage on each bar 
     use the matplotlib.pyplot.text() function
** Count missing data

***  use seaborn.barplot() see documentation for specific use
     this is for the use of summarized data, in other words
     necessary stats info is already gathered

** Pie Chart
   Show how data group proportion distribution is broken down into
   data group is no more than 4
   plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,
        counterclock = False);
   note, the data 'sorted_counts' need to be already summarized
   wedgeprops = {'width' : 0.4} -> argument in pie() that removes the core
   of the pie and thus creating a pie chart

** Histogram,
   it is like probability distributions
   sb.hist()
   change bin size either by providing a single value or a list of 
   equally spaced numbers

*** distplot 
    plots the distribution plot by default

** Figures , Axes, and Subplots

***   How a graph is drawn in matplotlib
      first you need to create figure object, then you create axes object, finally 
      you plot the graph in axes

***  To create axes 
     use add_axes()
     to use this axes, in countplot(...., ax = ax )
     plt.gca() get current axes
     fig.get_axes() get the list of axes
     fig.add_subplot()  approximately equal to plt.subplot() 
     plt.subplots() create multiple subplots

** Choosing a plot for discrete data

***  use rwidth parameter with plt.hist() 
     plot bars separately

**  Descriptive Stats, Outliers, AXis Limits

***  plt.xlim({a,b}) 
     zooms in to the histogram

** Scales and Transformations

***  log-normal distributed
     when the value is taken the log, the distribution is normally distributed
* <2018-10-20 Sat>

** Why use Pandas
   to analyze your data set, such that it matches your machine learning algo's
   requirement

** Creating Pandas Series

*** To import 
    import pandas as pd 

*** To create 
    var = pd.Series(data, index)

*** What is a series 
    one d array that holds many data types

***  series special points
     index of series do not just have to be numbers, they can be strings
     much like in a dictionary

*** What meta data about a series 
    var.shape 
    var.ndim : how many dimension
    var.size : how many elements in there 
    var.values
    var.index

** Access and deleting elements in pandas series

***  access through both index or number index 
     var[0] will return the first data in the series 
     but when there is ambiguity, i.e , when the index itself is also int
     use var.loc[index] to access via index 
     use var.iloc[int index] to access via integer index , i.e like a normal array

***  delete item 
     var.drop(index) : deletes the index withouth modifying original series
     and returns a modified array 
     var.drop(index, inplace  = True), modifies the original series

** Arithmetic Operations on pandas series

*** arithmetic operations 
    works just like in an ndarray in numpy
    but you have to take care that the operation you do must be compatible with 
    all data types in that series

** Creating Pandas Dataframes
  
*** what is a dataframe
    it is like a spreadsheet in excel 
    
*** How to create  
   pd.DataFrame(item)
   item is in the form of dictionary of pd Series
   the row indices will be the union of two indexs,
   for the item that does not contain some index, we will palce NaN as the value

**** What if we do not have index
     pandas use 0 ,1 ,2 .... as default value

**** What if we want to create only part of that dict
     bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])
     bob_shopping_cart = pd.DataFrame(items, columns=['Bob'])
     specify which item or index in the dict you want to create from

**** What if we want to specify row index oursleves
     df = pd.DataFrame(data, index = ['label 1', 'label 2', 'label 3'])

**** What if we want to create DataFrame via a list of dictionaries 
     dictionary keys will be the column indices 
     row indices will not be defined, so default values kicks in

** Access and adding elements in DataFrame

*** Access
    var[ [list of column labels] ]
    var.loc[ [list of row labels] ]
    var[column index][row index] to access single element 
    Alert: column always have to be places in front of row

*** Adding

**** To add new column 
     var['new_label'] = [list of data]

**** Add new column via adding up other columns 
     var['new_label'] = var['old_label_1'] + var['old_label_2']

**** To add new row
     First create new DataFrame 
     then use old_frame.append()

**** To add new column that is part of the existing column at the end
     store_items['new watches'] = store_items['watches'][start:end]

**** To insert new column anywhere
     dataframe.insert(loc,label,data)
     note the location starts at 0 which in the row labels

**** To remove items

***** pop('column_index') 
      deletes columns

***** drop(['index'], axis = 0/ row, 1 / column)
      deletes both rows and columns

**** To rename label 
     store_items = store_items.rename(columns = {'bikes': 'hats'}) : changes column
     store_items = store_items.rename(index = {'store 3': 'last store'}) : changes row
     store_items = store_items.set_index('pants') : set row index to be data in a column

** Deal with NaN value

*** Detect and count

**** .isnull() 
     returns the same shape of data that indicates whether each place is null or not by a boolean 
     use multiple .sum() to count how many True (which means is NaN) there is in the entire DataFram
     each .sum() reduces the dimensionality of the DataFrame by 1

****  .count() 
      counts the non-NaN values

****  .drop(axis = 0/row , 1 /column, inplace = boolean )
      delete all columns or rows that contains NaN
      notice this does not modify the original DataFrame by default, if want change the inplace value

****  .fillna(value)
      fill all NaN with value provided

****  .fillna(method = 'ffill', axis = 0 /column, 1 / row )
      fille NaN with the value before them along the axis specified

****  .interpolate(method = 'linear', axis = )

** Loading Data into a Pandas DataFrame

***  To load CSV file 
     pd.read_csv('file_name')

*** General Information
    file_name.tail(N) : last N rows is displayed
    file_name.head(N) : first N rows is displayed
    file_name.isnull().any() : check if any column had NaN values
    file_name.describe() : gives statistical description on some data
    file_name['column index'].describe() : describes a single column
    file_name.groupby() : collects data that has the same data in some columns 
    and then form a new DataFrame and does calculation on them
* <2018-10-19 Fri>

** Slicing ndarrays 
   X[start:end]
   X[start:]

*** Slicing only creates a new label -> the variable name, but not a new ndarray
    to create a new nparray, use copy()

***  to get diagonal 
     use np.diag(ndarray, k = N)
     N is the number of element above or below the diagonal

***  to get unique elements in the array
     np.unique

***  np.sort(x)
     leaves x unchanged

***  x.sort()
     changes the array x itself

***  access elements in an array that satsifies a boolean expression
     place the boolean expression in the index part
     eg: x[ bool expression]
     
*** np.sort(x,axis = ?)
    sort rank 2 arrays, the axis argument tells the program 
    whether to sort everything row wise or column.

** Arithmetic operations and Broadcasting

*** broadcasting 
    it allows you to do arithmetic operations of smaller size arrays
    to bigger ones
    behind the scene, python broadcasts the smaller array/ number into the same
    shape as the larger one 
    [[https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html][broadcasting rules]]

***  numpy also has awesome functions that obtian stats info of an ndarray 
* <2018-10-17 Wed>

** using anaconda
   conda install 'package_name'
   conda search 'package_name_approx'

** Creating and using  another encironment
   conda create -n env_name [list of packages] [python= version_number]
   source activate my_env
   source deactivate
   conda env export > environment.yaml : export the current enviroment into a file
   conda env create -f environment.yaml : load environment from a file
   conda env remove -n env_name : remove an environment

** Things about using an environment 
   create two env for python2 and python3 for general use
   pip freeze > requirements.txt : does the same job as conda env export 
   [[https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/][Extra Learning on Conda]]

** Jupyter note book 

*** Literate programming 
    documentation is written as a narrative alongside the code

*** How notebooks work 
    server renders notebook file and then send it via http&websockets to user
    the code part of the notebook is sent to the kernel
    kernel can not only interprete one language but many

*** jupyter short cuts
    shift+tab to have function documentation
    shift+tab continutously twice to bring up help document

*** Markdown cell style 
    Use #, or ##, or ### before text for different size of header
    [Text] (URL)
    _text_ or *text* to italics
    __text__ or **text** for bold 
    wrap code around with '''   '''
    or indent all code with 4 spaces
    For math block , wrap the entire block with $$  $$, then follow latex rules
    For math equation, wrap the equation with $ $
    
    [[https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet][Cheat sheet]]

*** Short cuts
    use Y to make a block code 
    use M to make a block markdown 
    use H to call out the help menu 
    use L to turn on and off code line number
    use D D to delete a cell
    shift + control + p to access control palette

*** Magic keywords
    gProbably only works in python kernel mode
    % magic word works for a line 
    %% magic word works for a cell
    example of magic word : timeit , times the code 
    The use of matplotlib inline to render a plot is not very clear, read more while coding 
    %pdb for debugging
    [[https://ipython.readthedocs.io/en/stable/interactive/magics.html][Magic word list]]

*** Convert notebooks
    use jupyter nbconvert --to file_format ipynb_file_name
    convert ipynb file into other format, because ipynb is json, so 
    jupyter nbconvert notebook.ipynb --to slides : convert to slides
    jupyter nbconvert notebook.ipynb --to slides --post serve : convert to slides and then serve

** Intro to Numpy

*** Why numpy
    numpy is faster than plain python if you use built in function in numpy
    numpy arrays can only hold one type of data at a time

***  Creating and saving numpy ndarrys
     np.array does up casting if the elements in the array are some ints and some floats to keep calculation precession
     x = np.array([1.5, 2.2, 3.7, 4.0, 5.9], dtype = np.int64) : assigns specific data type to the array
     np.save('my_array', x) : saves the ndarray 
     y = np.load('my_array.npy') : loads the ndarray

*** Use built-in functions to create ndarrays
    np.eye create identity matrix
    np.diag, create diagonal matrix
    np.full create array with specific dimension with specific value 
    np.arange create a linear array
    np.linspace require both start and end points
    np.reshape convert rank 1 array to another rank 2 array
    np.random.random ,random float nubmer array with specific shape
    np.random.randint ,
    np.random.normal, array with specific shape whose values follow normal distribution with specified distribution property
    np.zeros() creates zero array
    np.ones()

*** Accessing, Deleting and Inserting into ndarrays 
    use np.delete()
        np.insert()
	np.append()
	np.vstack() stack one array above another
	np.hstack() stack one array horizontally 
* <2018-10-16 Tue>

** Create one environment to each project 
   Use Conda, more specifically, : conda create

** Copy your current dependency for others to follow
   pip freeze > requirement.txt
* <2018-10-15 Mon>

** encapsulation
   Grouping different functions into a class
   This hides the implementation of different functions
** creater function
   __init__(arguments)
** self argument
   If you want to access attributes of a class, you would have to include
   the self as one argument
** 
* <2018-10-13 Sat>

**  Use argparse() to write user-friendly command line interface

** 
* <2018-10-12 Fri>
** Reading and Writing File
*** open("filename",mode of opening) returns a file object
****  this object that we operate them
**** if you forget to close file, you can run out of file handle thus no longer be able to open new files
****  if you open file with "w" mode, you delete everything it contains before
****  if you want to append use "a" mode
*** with .... as ..... 
**** with open('my_path/my_file.txt', 'r') as f:
**** automatically closes f outside the scope
****  but things decleared inside the with scope is not limited to exist before with .... as ends:
*** Use readline() to read line by line in python
*** use strip() to remove '\n'
** Import Local scripts
***  import "url to otherfiel/name of the file"
***  Note, when we import other file, any thing that is ran in that file will be run at the same time when we run our file
***  If you want to access object num in another  file : anotherfile, use : anotherfile.name
***  the same is with functions
***  use import lonenamefile as abrev, to simplify the calling function process
***  if there is executing block of code in a file, put them under the if __name__ = "__main__" block of code
**** or first put them under def  main(): then do if __name__ == "__main__" : main()
****  this ensures that the block of code will only be executed if the file is been called upon, not imported.
****  when being imported, the __name__  = nameoffile
****  if called directly upon, __name__ = "__main__"
**  [[https://docs.python.org/3/library/][Python Standard Library]]
***  random.choice()
****  choose random object from a collection of data
***  random.sample(container name, number)
****  pick randomly a number of objects from a container
** Techniques for Importing Modules
***  import just few functions
****  from module_name import object_name1, name 2, name 3 
      from module_name import object_name as abbrev
**  Third-party libraries
***  import third party libraries after standard library
***  include "requirement.txt" with yoour code so that collaborators know which libraries they need to install
***  include versions is good practise
***  use pip install -r requirement.txt to install these requirements
* <2018-10-08 Mon>
** Accessing Error Messages
***  use "except .... as var_name" to store the error message into a string
***  if you want to catch any exception in general use keyword "Exception"
** Scripting with Raw Input
*** eval(" a string") evaluates the string as a line of python code
** Errors and Exceptions
*** try statement: runs a line of code
***  except statement : if exception is raised, run the following code
***  else statement: in the same indent as except statement, if no exception is raised, run that
***  finally statement: same indent as try, it is excecuted no matter what the previous things do, even if you ask the previous things to close the program
*** [[https://stackoverflow.com/questions/11551996/why-do-we-need-the-finally-clause-in-python][Why do we need finally ]]
***  except (tuples of exception you want this to catch)
***  may even use multiple except for one try to act differently according to different errors
* <2018-10-07 Sun>
** Iterators and Generators
*** Iterables: OBJECTS that gives you one element at a time when operated on it properly
****  eg; list , the return value of enumerate
***  iterator : what is created by generators
**** it represents a stream of data , which is different from list, a collection of data
*** generators :
**** Like functions that return a list, instead uses key word "yield" and return an iterator
**** use generators instead of list because we can generate/access the wanted element one at a time thus puts less stress on memoery [[https://softwareengineering.stackexchange.com/questions/290231/when-should-i-use-a-generator-and-when-a-list-in-python/290235][Why Generator]]
*** sq_list = [x**2 for x in range(10)]  # this produces a list of squares            sq_iterator = (x**2 for x in range(10))  # this produces an iterator of squares
** Lambda function
***  put the following into where you need the lambda function to go,i.e as a parameter of another function
****  lambda "parameters.....": what you need to do with these parameters
****  If you actually need to call this function later, assign name to this lambda function : func_name = lambda parameter : operation
** Scope
*** If a function tries to modify a global variable or something that is defined outside of the func, error occurs
** Functions
*** def func_name(arguments):
*** You may also do this when calling a function func(para1 = 10, para2=5), this is called pass by name
* <2018-10-06 Sat>

** For loops

*** range(start = 0, stop, step =1), if sepcify two variable, the first variable is start
*** string: lower() -> change all character into lower
*** string: replace("c1","c2") replace c1 into c2 in the string called upon
***  if range(start,end) start > end , returns empty list
***  dict().items() return a tuple of key and value in the dictionary
** Break, Continue
*** break breaks out a loop entires
*** continue skips one iteration of a loop
** Zip and Enumerate
***  zip returns ITERATOR of the combined two lists, we need to use list() to convert the return value of zip into an actual list
***  *some_list unzips a list of tuples but you have to use it in conjunction with zip()
***  enumerate() returns both the index and item of an iterable data structure
*** to transpose a matrix do tuple(zip(*data)
** List comprehension
*** capitalized_cities = [city.title() for city in cities]
*** squares = [x**2 if x % 2 == 0 else x + 3 for x in range(9)]
*** passed = [name  for name in scores  if scores[name] >= 65  ]
* <2018-10-05 Fri>
** Lists and Membership Operators
*** If you use index -1 you get the last item, -2 second to last
*** let q3 be a list q3[3:6] slices 
*** python list can contain a mix of different data types
*** use key word "in" "not in" to determine whether a data is in a list or not
*** List is a mutable data strucvture  type but string is not
*** the other important quality is whether a data structure type is ordered or not
*** ordered or not depends on whether we can use the position of the element in a data structure to access them
** List Methods
*** Lists are likely to be pass by reference since one list which are pointed by two different variable names are mutated at the same time when one varies
*** for string, max operator compares the alphabetical order
*** sorted() sorts the data structure
*** string.join(..) joins string elements together connecting them with the string on which join is called upon
** Tuples
***  Like list but are immutable and ordered
** Sets
***  Unordered and unique elements, can create set from lists using set(list_name)
*** pop()
** Dictionary
*** store key and value pair
*** use "in" or ".get()" to check if a key is in the dict
*** dictionary keys must be immutable
*** can setup what if return if .get() fails to grab what you want
** Compund Data Structure
*** Can setup dictionary as value of another dictionary
* <2018-10-04 Thu>
** Integer and Floats
*** Use type(x) to look up the type of a variable
*** use int(x) to cast x into a data type
*** 
** String
*** String in python is immutable
*** + to combine string
*** * to multiply string
*** format() can be used to print designated outputs 
* <2018-10-03 Wed>
** Arithemetic operator 
*** to take power, use "**"
*** ^ does bitwise xor
*** "//" integer division, rounds the answer down 

























 





